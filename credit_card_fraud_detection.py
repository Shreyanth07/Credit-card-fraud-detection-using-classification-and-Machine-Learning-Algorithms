# -*- coding: utf-8 -*-
"""credit-card-fraud-detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JqYM23rJnoppxMmF_Rur5PWnZgMawh5S

## Introduction
This is a binary classification assignment which will mark the transactions either as fraudulent or non-fraudulent based on the trained model on the dataset

Credit card fraud occurs when unauthorized users get access to an individual's credit card information in order to make purchases, transactions, or open new accounts. A few examples of credit card fraud include account manipulation, new account, cloned cards, and cards-not-available. This unauthorized access occurs through phishing, skimming, and sharing the information by a user unknowingly at times. However, this type of fraud can be detected through artificial intelligence and machine learning from prevention by issuers, institutions, and individual cardholders. According to 2021's annual report, about 50% of Americans have experienced a fraudulent charge on their credit or debit cards, and more than 1/3 credit or debit card holders have experienced fraud multiple times. This amounts to 127 million people in the US that have been victims of credit card theft for at-least once.
"""

# Import packages and libraries required

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sb
sb.set_theme(style='whitegrid')

from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

# Linear Algorithms
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# Non-linear Algorithms
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# Metrics to evaluate performance
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

# Load the dataset as dataframe
df = pd.read_csv('/content/credit_card_details.csv')
df.head(10)

# Summarize the dataset and transpose() to see all columns
df.describe(include='all').transpose()

# Check for null values
df.isnull().sum()

# Check for data types
df.dtypes

# Converting class feature into object for classification
df['class'] = df['class'].astype('object')
df.dtypes

# Distribution of the numerical features
df.plot(kind='density', subplots=True, layout=(8,4), sharex=False, legend=True, 
                                            fontsize=1, figsize=(16,24))
plt.show()

# Subset of dataset to check for normal distribution
sb.countplot(x=df['class'])

print(df['class'].value_counts())

no_fradulent = (df['class'].value_counts()[0] / len(df['class'])) * 100
fradulent = (df['class'].value_counts()[1] / len(df['class'])) * 100
print('\nRatio of the transactions which are not fradulent: %.2f%%' % no_fradulent)
print('Ratio of the transactions which are fradulent: %.2f%%' % fradulent)

# Random undersampling or oversampling using SMOTE to balance the dataset
df.drop('class', axis=1).plot(kind='box', subplots=True, layout=(8,4), sharex=False, 
                             legend=True, fontsize=8, figsize=(16,24))
plt.show()

# Correlation
fig = plt.figure(figsize=(12,12))
ax = fig.add_subplot(111)
cax = ax.matshow(df.corr(), vmin=-1, vmax=1, interpolation='none')
ax.grid(False)
fig.colorbar(cax)
ticks = np.arange(0, len(df.columns), 1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(df.columns, rotation=90)
ax.set_yticklabels(df.columns)
plt.show()

"""## Algorithm Evaluation"""

# Constants for training and evaluation options
n_fold = 10
scr = 'accuracy'

# Create a function that evaluates different algorithms can be used for each update on the model
def evaluate_algorithms(x, y):
    models = []
    models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))
    models.append(('LDA', LinearDiscriminantAnalysis()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    models.append(('SVM', SVC(gamma='auto')))
    
    names = []
    results = []
    for name, model in models:
        kfold = StratifiedKFold(n_splits=n_fold, random_state=seed, shuffle=True)
        cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scr)
        results.append(cv_results)
        names.append(name)
        msg = '%s - Mean ACC: %.2f%% STD(%.2f)' % (name, cv_results.mean()*100, cv_results.std())
        print(msg)
        
     # Plot the results
    fig = plt.figure(figsize=(8,8))
    fig.suptitle('Algorithm Comparison', fontsize=16, y=0.93)
    ax = fig.add_subplot(111)
    plt.boxplot(results)
    ax.set_xticklabels(names, fontsize=14)
    plt.show()

"""## Data Preparation"""

# Drop duplicates
nr_rows = df.shape[0]
print('Number of rows: %d' % nr_rows)
df = df.drop_duplicates().reset_index(drop=True)
print('%d duplicates removed' % (nr_rows - df.shape[0]))
print('Number of rows after removal of duplicates: %d' % (df.shape[0]))

# Split the data to Train-Test
seed = 101
x = df.drop('class', axis=1)
y = df['class']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=seed)

# Evaluating algorithms for the baseline
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_train_cat = label_encoder.fit_transform(y_train)
evaluate_algorithms(x_train, y_train_cat)

"""### Observations

Even if LDA provide 99.94% accuracy, the results are not very good. The dataset is imbalanced and it's easy to guess a transaction as non-fraudulent. Hence we need to use under or over sampling techniques to make the dataset balanced, then train the algorithms.

## Random Under Sampling
"""

from imblearn.under_sampling import RandomUnderSampler

# Number of the items in the minority class will be 50% of the majority class
under_sampler = RandomUnderSampler(sampling_strategy=0.5)
x_under, y_under = under_sampler.fit_resample(x_train, y_train_cat)
print('Under sampled shape of X: ', x_under.shape)
print('Under sampled shape of y: ', y_under.shape)

# Ratio of classes after undersampling
pd.DataFrame(y_under).value_counts()

evaluate_algorithms(x_under, y_under)

"""### Observations

Logistic Regression has the highest accuracy score and it would be the right fit for hyper parameter tuning. However, it's better to evaluate the performance of the algorithms without outliers and with scaled data.

## Outlier Detection
"""

x_under.plot(kind='box', subplots=True, layout=(8,4), sharex=False, 
                             legend=True, fontsize=8, figsize=(16,24))
plt.show()

# Detect outliers by using Interquartile Range Method (iqr)
for column in x_under.columns:
    # Calculate interquartile range
    q25 = np.percentile(x_under[column], 25)
    q75 = np.percentile(x_under[column], 75)
    iqr = q75 - q25
    print('Percentiles of %s: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (column, q25, q75, iqr))
    # Calculate the outlier cutoff value
    cut_off = iqr * 1.5
    lower_bound = q25 - cut_off
    upper_bound = q75 + cut_off
    print('%s: Lower = %.2f, Upper = %.2f' % (column, lower_bound, upper_bound))
    print('----------------------------')

# Merge the undersampled features dataframe with label numpy array to filter by upper and lower bouds detected above.
df_under = pd.concat([x_under, pd.DataFrame(y_under, columns=['class'])], axis=1)
df_under = df_under.reset_index(drop=True)
df_under.head(10)

# Remove the outliers based on the lower and upper bounds and no need to apply it to the Time feature as it does not has any outliers

df_no_outlier = df_under[((df_under['V1'] >= -6.43) & (df_under['V1'] <= 5.70) & 
                        (df_under['V2'] >= -3.48) & (df_under['V2'] <= 4.98) &
                        (df_under['V3'] >= -8.15) & (df_under['V3'] <= 5.92) & 
                        (df_under['V4'] >= -5.61) & (df_under['V4'] <= 7.96) & 
                        (df_under['V5'] >= -3.88) & (df_under['V5'] <= 3.15) & 
                        (df_under['V6'] >= -3.35) & (df_under['V6'] <= 2.30) &
                        (df_under['V7'] >= -4.82) & (df_under['V7'] <= 3.67) & 
                        (df_under['V8'] >= -1.47) & (df_under['V8'] <= 1.9) &
                        (df_under['V9'] >= -4.3) & (df_under['V9'] <= 3.13) & 
                        (df_under['V10'] >= -6.94) & (df_under['V10'] <= 4.51) & 
                        (df_under['V11'] >= -4.39) & (df_under['V11'] <= 6) & 
                        (df_under['V12'] >= -8.24) & (df_under['V12'] <= 5.57) &
                        (df_under['V13'] >= -2.91) & (df_under['V13'] <= 2.8) & 
                        (df_under['V14'] >= -11.8) & (df_under['V14'] <= 7.1) &
                        (df_under['V15'] >= -2.52) & (df_under['V15'] <= 2.59) & 
                        (df_under['V16'] >= -5.2) & (df_under['V16'] <= 3.77) &
                        (df_under['V17'] >= -4.01) & (df_under['V17'] <= 2.93) & 
                        (df_under['V18'] >= -3.09) & (df_under['V18'] <= 2.55) &
                        (df_under['V19'] >= -2.28) & (df_under['V19'] <= 2.58) & 
                        (df_under['V20'] >= -0.99) & (df_under['V20'] <= 1.13) &
                        (df_under['V21'] >= -1.14) & (df_under['V21'] <= 1.35) & 
                        (df_under['V22'] >= -2.22) & (df_under['V22'] <= 2.19) &
                        (df_under['V23'] >= -0.84) & (df_under['V23'] <= 0.8) & 
                        (df_under['V24'] >= -1.55) & (df_under['V24'] <= 1.54) &
                        (df_under['V25'] >= -1.38) & (df_under['V25'] <= 1.44) & 
                        (df_under['V26'] >= -1.2) & (df_under['V26'] <= 1.26) &
                        (df_under['V27'] >= -0.61) & (df_under['V27'] <= 0.81) & 
                        (df_under['V28'] >= -0.37) & (df['V28'] <= 0.46) &
                        (df_under['Amount'] >= 0) & (df_under['Amount'] <= 228.08))]

df_no_outlier.shape

df_no_outlier.drop('class', axis=1).plot(kind='box', subplots=True, layout=(8,4), sharex=False, 
                             legend=True, fontsize=8, figsize=(16,24))
plt.show()

# Impact on the balance of the dataset.
print(df_no_outlier['class'].value_counts())
sb.countplot(x=df_no_outlier['class'])

"""## Scaling Data"""

# Standard scaling for impact on algorithms
scaler = StandardScaler()
fit = scaler.fit(x_under)
x_scaled = fit.transform(x_under)

evaluate_algorithms(x_scaled, y_under)

"""Scaling the features improved the accuracy of the algorithm about 3% when compared to the previous test. Then, Logistic Regression provides the best acuracy score of 94.88%.

## Feature Selection using Feature Importance
"""

from sklearn.ensemble import ExtraTreesClassifier

model = ExtraTreesClassifier(n_estimators=100)
model.fit(x_scaled, y_under)
array = model.feature_importances_
print('Mean: %.3f, STD: %.3f' % (array.mean(), array.std()))
for i in range(0, len(array)):
    print ('Feature %d: %.3f' % (i, array[i]))

# Highest values selected
evaluate_algorithms(x_scaled[:, [4, 12, 14, 16, 17]], y_under)

import warnings
warnings.filterwarnings('ignore')

# Hyper-parameter Tuning
h_params = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
              'penalty': ['none', 'l1', 'l2', 'elasticnet'],
              'C': [100, 10, 1.0, 0.1, 0.01]}
model = LogisticRegression()

kfold = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)
grid = GridSearchCV(estimator=model, param_grid=h_params, cv=kfold)
grid.fit(x_scaled, y_under)

print('Best score: %.2f%%' % (grid.best_score_*100))
print('Best params; ', grid.best_params_)

"""The best hyper-parameter configuration is printed and with this config the model can achieve 95.40% of accuracy score. We'll train and test the model using these hyper-parameters.

## Test Model
"""

# To test the model, first need to train it.
lr = LogisticRegression(C=100, penalty='none', solver='newton-cg')
lr.fit(x_scaled, y_under)

predictions = lr.predict(fit.transform(x_test))
y_test_cat = label_encoder.fit_transform(y_test)
print('Accuracy score: %.2f%%' % (accuracy_score(y_test_cat, predictions) * 100))
print(classification_report(y_test_cat, predictions))

# Plot the confusion matrix
cf_matrix = confusion_matrix(y_test_cat, predictions)
fig = plt.subplots(figsize=(10, 8))
sb.set(font_scale=1.6)
sb.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.1%')
plt.xlabel('Predicted Label', fontsize=14)
plt.xticks(fontsize=14)
plt.ylabel('True Label', fontsize=14)
plt.yticks(fontsize=14)
plt.show()

"""## Conclusion

The purpose of this project is to create a model that can detect the fraudulent transactions based on the given features. The given dataset was in highly imbalanced state. Split the dataset into train and test where testhaving 20%. Have taken 5 algorithms with the current state of the dataset to set a baseline. The results were shown to be deceptive.

Balanced the dataset by using Random Under-Sampler feature and created a sample dataset with majority class having 50% more observations than the minority class.

Applied standard scaler as some of the features have wide range of minimum and maximum values. Scaling imporved the results slightly and removing outliers caused imbalanced dataset. Standard Scaler was fitted on the training dataset to make sure for no dataleak.

Tried to reduce the number of features by using Feature Importance by adapting Extra Tree Classifier algorithms but unfortunately it had a negative impact on the results.

After deciding how to process the data and the algorithm, the best hyper-parameters using Grid Search Cross Validation on Logistic Regression provided the best accuracy score on a dataset which was balanced and scaled.

Trained the model one last time with the selected hyper-parameters and then tested it on the test dataset. The accuracy score of the proposed model is 98.43% which is a accurate value to detect fraudelent credit card transactions.

Different types of feature selection methods and/or scaling methods are applied to improve the performance. Dataset can also be balanced using SMOTE and the performance of the model can be tested under big amount of observations.
"""